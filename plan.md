# Plan: Cloudflare Workers AI "Ask" Feature (Additional Option)

## Goal
Add a **second** LLM option to the existing "ask" page: a Cloudflare Worker calling
Llama 3.1 8B via Workers AI. The existing in-browser LLM (distilgpt2 via Transformers.js)
remains fully functional. Users choose which engine to use. Deploy the Worker
infrastructure with Terraform. Result: "I deployed an LLM to a production environment."

---

## Current State

- **Frontend** (`site/ask/index.html`): Loads `embeddings.json`, computes query
  embedding with `Xenova/all-MiniLM-L6-v2`, does cosine similarity, generates answer
  with `Xenova/distilgpt2` -- all in-browser via Transformers.js.
- **Embeddings pipeline** (`site/generate_embeddings.py`): At build time, chunks all
  blog posts, generates embeddings with `all-MiniLM-L6-v2`, writes
  `_site/static/model/embeddings.json`.
- **Deployment**: GitHub Actions builds Jekyll site and deploys to GitHub Pages.

## Target State

- **Existing in-browser path preserved** -- no changes to Transformers.js code or
  the `all-MiniLM-L6-v2` embeddings.
- **New Cloudflare Worker** as an additional backend option, handling the full RAG
  pipeline server-side with its own embedding model (`bge-small-en-v1.5`) and text
  generation model (`llama-3.1-8b-instruct`).
- **KV namespace** stores a separate set of pre-computed embeddings compatible with
  the Workers AI embedding model.
- **Terraform** manages all Cloudflare infrastructure.
- **Frontend toggle** lets users choose between "In-Browser" and "Cloud" modes.

---

## Architecture

```
Browser (ask page)
  |
  |-- [Toggle: "In-Browser" mode] --> existing Transformers.js RAG (unchanged)
  |
  |-- [Toggle: "Cloud" mode]
        |
        | POST /ask  { "question": "..." }
        v
      Cloudflare Worker (ask-ai.<account>.workers.dev)
        |
        |-- 1. Compute query embedding via Workers AI (bge-small-en-v1.5)
        |-- 2. Load blog embeddings from KV namespace
        |-- 3. Cosine similarity search (top 3 chunks)
        |-- 4. Build RAG prompt with retrieved context
        |-- 5. Generate answer via Workers AI (llama-3.1-8b-instruct)
        |
        v
      JSON response { "answer": "...", "sources": [...] }
```

**Why these models for the Cloud path:**
- `@cf/baai/bge-small-en-v1.5` -- 384-dim embeddings, good quality, low neuron cost.
- `@cf/meta/llama-3.1-8b-instruct` -- massive quality upgrade over distilgpt2,
  follows instructions well, fits within free tier neuron budget.

**Why two separate embedding sets:**
- The in-browser path uses `all-MiniLM-L6-v2` embeddings.
- The Worker path uses `bge-small-en-v1.5` embeddings.
- Different models produce vectors in different embedding spaces -- they are not
  interchangeable. Each path needs embeddings generated by its matching model.

**Free tier budget:**
- 10,000 neurons/day (Workers AI), 100,000 requests/day (Workers), KV free tier
  gives 100,000 reads/day. More than sufficient for a personal site.

---

## Monorepo Directory Structure

New files to add (existing files marked with MODIFIED):

```
garretfick.github.io/
+-- workers/
|   +-- ask-ai/
|       +-- src/
|       |   +-- index.js          # Worker entry point
|       +-- package.json          # For local dev tooling (wrangler)
|       +-- wrangler.toml         # For local dev/testing only
+-- terraform/
|   +-- main.tf                   # Provider, worker script, KV, AI binding
|   +-- variables.tf              # account_id, api_token, etc.
|   +-- outputs.tf                # Worker URL output
|   +-- terraform.tfvars.example  # Example values (no secrets)
+-- site/
    +-- generate_embeddings.py    # MODIFIED: generates TWO embedding files
    +-- ask/
        +-- index.html            # MODIFIED: add toggle + cloud mode fetch logic
```

---

## Implementation Steps

### Step 1: Create the Cloudflare Worker (`workers/ask-ai/src/index.js`)

A single Worker script that:

1. **Handles CORS** -- `OPTIONS` preflight + `Access-Control-Allow-Origin` header
   scoped to the GitHub Pages origin (`https://garretfick.github.io`).
2. **Accepts POST /ask** with JSON body `{ "question": "..." }`.
3. **Computes query embedding** by calling `env.AI.run("@cf/baai/bge-small-en-v1.5", ...)`.
4. **Loads embeddings from KV** -- single key `embeddings` containing the full JSON
   array of `{ chunk, embedding }` objects.
5. **Cosine similarity search** -- rank all chunks, take top 3.
6. **Builds RAG prompt** -- system message constraining the model to answer only from
   context, plus the user question and retrieved chunks.
7. **Generates answer** by calling `env.AI.run("@cf/meta/llama-3.1-8b-instruct", ...)`.
8. **Returns JSON** `{ "answer": "...", "sources": ["chunk1...", "chunk2...", "chunk3..."] }`.
9. **Error handling** -- returns structured JSON errors with appropriate HTTP status codes.

Add `workers/ask-ai/package.json` with `wrangler` as a dev dependency for local
testing. Add `workers/ask-ai/wrangler.toml` for `wrangler dev` (local development
only; production deployment is via Terraform).

### Step 2: Create Terraform Configuration (`terraform/`)

**`terraform/main.tf`:**

```hcl
terraform {
  required_providers {
    cloudflare = {
      source  = "cloudflare/cloudflare"
      version = "~> 5.0"
    }
  }
}

provider "cloudflare" {
  api_token = var.cloudflare_api_token
}

# KV namespace for storing embeddings
resource "cloudflare_workers_kv_namespace" "ask_ai_embeddings" {
  account_id = var.cloudflare_account_id
  title      = "ask-ai-embeddings"
}

# Worker script with AI and KV bindings
resource "cloudflare_workers_script" "ask_ai" {
  account_id  = var.cloudflare_account_id
  script_name = "ask-ai"
  content     = file("${path.module}/../workers/ask-ai/src/index.js")

  metadata = {
    main_module        = "index.js"
    compatibility_date = "2025-01-01"
    bindings = [
      {
        type = "ai"
        name = "AI"
      },
      {
        type           = "kv_namespace"
        name           = "EMBEDDINGS_KV"
        namespace_id   = cloudflare_workers_kv_namespace.ask_ai_embeddings.id
      }
    ]
  }
}

# Upload embeddings to KV
resource "cloudflare_workers_kv" "embeddings_data" {
  account_id   = var.cloudflare_account_id
  namespace_id = cloudflare_workers_kv_namespace.ask_ai_embeddings.id
  key_name     = "embeddings"
  value        = file("${path.module}/../site/_site/static/model/embeddings-bge.json")
}
```

**`terraform/variables.tf`:**
- `cloudflare_api_token` (sensitive)
- `cloudflare_account_id`
- `allowed_origin` (default: `https://garretfick.github.io`)

**`terraform/outputs.tf`:**
- Worker URL (`ask-ai.<account>.workers.dev`)

**`terraform/terraform.tfvars.example`:**
- Template with placeholder values, no secrets.

### Step 3: Generate Dual Embeddings (`site/generate_embeddings.py`)

Modify the existing script to generate **two** embedding files:

1. `_site/static/model/embeddings.json` -- using `all-MiniLM-L6-v2` (existing,
   for the in-browser path, unchanged behavior).
2. `_site/static/model/embeddings-bge.json` -- using `BAAI/bge-small-en-v1.5`
   (new, for the Cloudflare Worker path).

Both models output 384-dim vectors. The chunking logic stays the same -- only the
encoding step runs twice with different models. The same chunks get embedded by both
models.

### Step 4: Update the Ask Page Frontend (`site/ask/index.html`)

Add a mode toggle while preserving the existing Transformers.js code:

- **Add a toggle/select** above the question input: "In-Browser (experimental)" vs
  "Cloud (Llama 3.1)".
- **In-Browser mode**: Runs the existing Transformers.js pipeline exactly as today.
  No changes to this code path.
- **Cloud mode**: On button click, POST to the Worker URL with
  `{ "question": questionInput.value }`, show loading state, display the returned
  `answer` field.
- The Worker URL can be hardcoded since this is a personal site.
- Default to "Cloud" mode since it's faster and produces better answers, but let
  users try the in-browser mode too.

### Step 5: Update CI/CD Pipeline (`.github/workflows/deploy.yaml`)

Add a job (or extend the existing one) that runs after the Jekyll build:

1. **Setup Terraform** -- use `hashicorp/setup-terraform` action.
2. **Terraform init + apply** -- deploys the Worker and KV namespace. Uses secrets
   `CLOUDFLARE_API_TOKEN` and `CLOUDFLARE_ACCOUNT_ID` from GitHub repo settings.
3. The embeddings upload happens implicitly via the `cloudflare_workers_kv` Terraform
   resource that reads the built `embeddings-bge.json`.

**Sequencing:**
```
Jekyll build (generates both embeddings.json and embeddings-bge.json)
  -> Terraform apply (deploys Worker + uploads bge embeddings to KV)
  -> GitHub Pages deploy (deploys updated frontend with toggle)
```

### Step 6: Update .gitignore and Dev Container

- Add `terraform/.terraform/`, `terraform/*.tfstate*`, `terraform/.terraform.lock.hcl`
  to `.gitignore`.
- Add `BAAI/bge-small-en-v1.5` to the Python dependencies in the dev container
  Dockerfile and GitHub Actions workflow (sentence-transformers already handles this;
  the model downloads automatically on first use).

---

## Key Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Relationship to existing | Additive, not replacement | User requirement; both options coexist |
| Dual embeddings | Two files, two models | Different embedding models produce incompatible vector spaces |
| Embedding storage | KV (single key) | Simple, free tier allows 100K reads/day, embeddings fit in one value (25MB limit) |
| Cloud embedding model | bge-small-en-v1.5 | Available on both Workers AI and locally via sentence-transformers, 384-dim |
| Text generation model | Llama 3.1 8B Instruct | Huge quality upgrade over distilgpt2, instruction-following, fits free tier |
| IaC tool | Terraform (not Wrangler) | User requirement; demonstrates IaC skills for interviews |
| CORS strategy | Allowlist GitHub Pages origin | Security best practice; no wildcard |
| Default mode | Cloud | Better answers, faster page load; in-browser remains available |
| Worker URL | `*.workers.dev` subdomain | Free, no custom domain needed, avoids DNS complexity |

---

## What Makes This Interview-Ready

1. **Production LLM deployment** -- Llama 3.1 8B running on Cloudflare's global edge
   network, serving real traffic.
2. **Infrastructure as Code** -- Full Terraform configuration, reproducible deployments.
3. **RAG architecture** -- Semantic search over real content with retrieval-augmented
   generation. Industry-standard pattern.
4. **CI/CD pipeline** -- Automated build, embed, deploy cycle via GitHub Actions.
5. **Cost engineering** -- Designed to run entirely within free tier limits.
6. **Production concerns** -- CORS security, error handling, structured API responses.
7. **Side-by-side comparison** -- Users can compare in-browser vs cloud LLM quality,
   which makes for a compelling demo.

---

## Risks and Mitigations

| Risk | Mitigation |
|------|------------|
| Embeddings JSON too large for KV value (>25MB) | ~155 posts with 384-dim floats should be ~2-5MB. Monitor size. |
| 10ms CPU time limit on free tier | Cosine similarity over ~500 chunks should be well under 10ms. AI calls are I/O wait, not CPU. |
| 10,000 neurons/day limit | Personal site traffic is low. Each query costs ~100-200 neurons (embedding + generation). Budget for ~50-100 queries/day. |
| Terraform Cloudflare provider v5 API changes | Pin provider version in `required_providers`. |
| Dual embedding generation doubles build time | Embedding generation is fast (~30s per model). Acceptable tradeoff. |
| `bge-small-en-v1.5` local vs Workers AI slight differences | Both use the same BAAI model weights. Test similarity scores after deployment. |

---

## Out of Scope (for now)

- Streaming responses (would require SSE/WebSocket -- keep it simple with JSON)
- Custom domain for the Worker (workers.dev is fine)
- Rate limiting beyond Cloudflare's built-in free tier limits
- Vectorize (Cloudflare's vector DB) -- KV is simpler and sufficient for this scale
- Caching responses -- could add later with Workers Cache API
