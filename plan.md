# Plan: Cloudflare Workers AI "Ask" Feature (Additional Option)

## Goal
Add a **second** LLM option to the existing "ask" page: a Cloudflare Worker calling
Llama 3.1 8B via Workers AI. The existing in-browser LLM (distilgpt2 via Transformers.js)
remains fully functional. Users choose which engine to use. Deploy the Worker
infrastructure with Terraform. Result: "I deployed an LLM to a production environment."

---

## Current State

- **Frontend** (`site/ask/index.html`): Loads `embeddings.json`, computes query
  embedding with `Xenova/all-MiniLM-L6-v2`, does cosine similarity, generates answer
  with `Xenova/distilgpt2` -- all in-browser via Transformers.js.
- **Embeddings pipeline** (`site/generate_embeddings.py`): At build time, chunks all
  blog posts, generates embeddings with `all-MiniLM-L6-v2`, writes
  `_site/static/model/embeddings.json`.
- **Deployment**: GitHub Actions builds Jekyll site and deploys to GitHub Pages.

## Target State

- **Existing in-browser path preserved** -- no changes to Transformers.js code or
  the `all-MiniLM-L6-v2` embeddings.
- **New Cloudflare Worker** as an additional backend option, handling the full RAG
  pipeline server-side with its own embedding model (`bge-small-en-v1.5`) and text
  generation model (`llama-3.1-8b-instruct`).
- **KV namespace** stores a separate set of pre-computed embeddings compatible with
  the Workers AI embedding model.
- **Terraform** manages all Cloudflare infrastructure.
- **Frontend toggle** lets users choose between "In-Browser" and "Cloud" modes.

---

## Architecture

```
Browser (ask page)
  |
  |-- [Toggle: "In-Browser" mode] --> existing Transformers.js RAG (unchanged)
  |
  |-- [Toggle: "Cloud" mode]
        |
        | POST /ask  { "question": "..." }
        v
      Cloudflare Worker (ask-ai.<account>.workers.dev)
        |
        |-- 1. Compute query embedding via Workers AI (bge-small-en-v1.5)
        |-- 2. Load blog embeddings from KV namespace
        |-- 3. Cosine similarity search (top 3 chunks)
        |-- 4. Build RAG prompt with retrieved context
        |-- 5. Generate answer via Workers AI (llama-3.1-8b-instruct)
        |
        v
      JSON response { "answer": "...", "sources": [...] }
```

**Why these models for the Cloud path:**
- `@cf/baai/bge-small-en-v1.5` -- 384-dim embeddings, good quality, low neuron cost.
- `@cf/meta/llama-3.1-8b-instruct` -- massive quality upgrade over distilgpt2,
  follows instructions well, fits within free tier neuron budget.

**Why two separate embedding sets:**
- The in-browser path uses `all-MiniLM-L6-v2` embeddings.
- The Worker path uses `bge-small-en-v1.5` embeddings.
- Different models produce vectors in different embedding spaces -- they are not
  interchangeable. Each path needs embeddings generated by its matching model.

**Free tier budget:**
- 10,000 neurons/day (Workers AI), 100,000 requests/day (Workers), KV free tier
  gives 100,000 reads/day. More than sufficient for a personal site.

---

## Monorepo Directory Structure

New files to add (existing files marked with MODIFIED):

```
garretfick.github.io/
+-- workers/
|   +-- ask-ai/
|       +-- src/
|       |   +-- index.js          # Worker entry point
|       +-- package.json          # For local dev tooling (wrangler)
|       +-- wrangler.toml         # For local dev/testing only
+-- terraform/
|   +-- main.tf                   # Provider, worker script, KV, AI binding
|   +-- variables.tf              # account_id, api_token, etc.
|   +-- outputs.tf                # Worker URL output
|   +-- terraform.tfvars.example  # Example values (no secrets)
+-- site/
    +-- generate_embeddings.py    # MODIFIED: generates TWO embedding files
    +-- ask/
        +-- index.html            # MODIFIED: add toggle + cloud mode fetch logic
```

---

## Implementation Steps

Each step is a single commit. After every commit, the Jekyll build (`just build`)
must pass and the existing ask page must continue to work in-browser.

**Build chain reminder:** `just build` runs `bundle exec jekyll build`, which
triggers `site/_plugins/generate_embeddings.rb` (a post-write hook) that calls
`python3 ./generate_embeddings.py`. The hook does **not** fail the build if the
Python script errors, but we should keep it working anyway.

---

### Step 1: .gitignore and Dev Container Prep

**Files changed:** `.gitignore`, `.devcontainer/Dockerfile`

Add ignore rules for files that will be created in later steps:
- `terraform/.terraform/`
- `terraform/*.tfstate*`
- `terraform/.terraform.lock.hcl`
- `workers/ask-ai/node_modules/`

Update dev container Dockerfile: no changes needed for Python deps
(`sentence-transformers` already installed; `BAAI/bge-small-en-v1.5` auto-downloads
on first use). Optionally add `terraform` and `node` if not already present.

**Why first:** Sets up ignore rules before creating the files, so nothing
accidentally gets committed. Pure housekeeping.

**Build verification:**
```
just build                    # Jekyll build + embeddings must pass
# Ask page works in-browser   (unchanged)
```

---

### Step 2: Cloudflare Worker

**Files added:** `workers/ask-ai/src/index.js`, `workers/ask-ai/package.json`,
`workers/ask-ai/wrangler.toml`

Create the Worker script that handles the full server-side RAG pipeline:

1. **CORS handling** -- `OPTIONS` preflight + `Access-Control-Allow-Origin` scoped
   to `https://garretfick.github.io`.
2. **POST /ask** with JSON body `{ "question": "..." }`.
3. **Query embedding** via `env.AI.run("@cf/baai/bge-small-en-v1.5", ...)`.
4. **Load embeddings from KV** -- key `embeddings`, JSON array of `{ chunk, embedding }`.
5. **Cosine similarity** -- rank chunks, take top 3.
6. **RAG prompt** -- system message + context + user question.
7. **Generate answer** via `env.AI.run("@cf/meta/llama-3.1-8b-instruct", ...)`.
8. **Return JSON** `{ "answer": "...", "sources": [...] }`.
9. **Error handling** -- structured JSON errors with HTTP status codes.

Add `package.json` with `wrangler` as a dev dependency (for local testing).
Add `wrangler.toml` for `wrangler dev` (local dev only; production via Terraform).

**Why this step is safe:** Entirely new files in `workers/` directory. Not part of
the Jekyll build. The site doesn't reference these files yet.

**Build verification:**
```
just build                    # Jekyll build + embeddings must pass (unaffected)
node -c workers/ask-ai/src/index.js   # Syntax check the Worker JS
# Ask page works in-browser   (unchanged)
```

---

### Step 3: Terraform Configuration

**Files added:** `terraform/main.tf`, `terraform/variables.tf`,
`terraform/outputs.tf`, `terraform/terraform.tfvars.example`

Terraform config for:
- Cloudflare provider (pinned `~> 5.0`)
- `cloudflare_workers_kv_namespace` for embeddings storage
- `cloudflare_workers_script` with AI + KV bindings (references Worker JS from Step 2)
- `cloudflare_workers_kv` to upload `embeddings-bge.json` (file won't exist yet --
  that's fine, Terraform only reads it at `plan`/`apply` time, not at commit time)
- Variables: `cloudflare_api_token` (sensitive), `cloudflare_account_id`, `allowed_origin`
- Outputs: Worker URL
- `terraform.tfvars.example` with placeholder values (no secrets)

**Why this step is safe:** Entirely new files in `terraform/` directory. Not part of
the Jekyll build. Terraform doesn't run until CI is configured (Step 6).

**Build verification:**
```
just build                    # Jekyll build + embeddings must pass (unaffected)
terraform fmt -check terraform/   # Format check (if terraform installed)
# Ask page works in-browser   (unchanged)
```

---

### Step 4: Dual Embedding Generation

**Files changed:** `site/generate_embeddings.py`

Modify the existing script to generate **two** embedding files from the same chunks:

1. `_site/static/model/embeddings.json` -- using `all-MiniLM-L6-v2` (existing
   behavior, unchanged output for the in-browser path).
2. `_site/static/model/embeddings-bge.json` -- using `BAAI/bge-small-en-v1.5`
   (new, for the Cloudflare Worker path uploaded to KV via Terraform).

Implementation approach:
- Keep the existing chunking logic untouched.
- Load both models at the start.
- After chunking, encode once per model.
- Write both output files.
- The existing `embeddings.json` output is byte-for-byte identical to before.

Both models output 384-dim vectors. Same chunks, two encoding passes.

**Why this step is safe:** The existing `embeddings.json` is still generated with the
same model and same logic. The ask page loads `embeddings.json` and works exactly as
before. The new `embeddings-bge.json` is an extra output that nothing references yet.

**Build verification:**
```
just build                    # Jekyll build must pass
ls site/_site/static/model/   # Both embeddings.json AND embeddings-bge.json exist
python3 -c "import json; d=json.load(open('site/_site/static/model/embeddings.json')); print(f'{len(d)} chunks')"
python3 -c "import json; d=json.load(open('site/_site/static/model/embeddings-bge.json')); print(f'{len(d)} chunks')"
# Both files should have the same number of chunks
# Ask page works in-browser   (unchanged -- still loads embeddings.json)
```

---

### Step 5: Frontend Toggle

**Files changed:** `site/ask/index.html`

Add a mode selector while preserving all existing Transformers.js code:

- **Add a `<select>` or radio buttons** above the question input:
  "In-Browser (distilgpt2)" vs "Cloud (Llama 3.1 8B)".
- **In-Browser mode**: Executes the existing Transformers.js pipeline. Zero changes
  to this code path. All existing JS stays in place.
- **Cloud mode**: On submit, POST to the Worker URL with
  `{ "question": questionInput.value }`. Show "Thinking..." status. Display the
  returned `answer`. Show source chunks if returned.
- **Worker URL**: Hardcoded placeholder (e.g., `https://ask-ai.<account>.workers.dev`).
  Will be updated once the Worker is deployed. Until then, Cloud mode shows a
  user-friendly error ("Cloud service not yet deployed" or similar).
- **Default to In-Browser** for now (since the Worker isn't deployed yet). Switch
  default to Cloud after Step 6 deploys the Worker.
- **Loading behavior**: In-Browser mode still downloads Transformers.js models on
  first use (existing behavior). Cloud mode skips model downloads entirely.

**Why this step is safe:** The existing in-browser code path is untouched. The
toggle defaults to In-Browser. Cloud mode gracefully handles the Worker not being
available. The page loads, the existing flow works.

**Build verification:**
```
just build                    # Jekyll build must pass
# Open ask page in browser:
#   - Select "In-Browser" -> ask a question -> works as before
#   - Select "Cloud" -> ask a question -> shows friendly error (Worker not deployed)
```

---

### Step 6: CI/CD Pipeline

**Files changed:** `.github/workflows/deploy.yaml`

Add a Terraform deployment step **between** the Jekyll build and GitHub Pages deploy:

1. **Setup Terraform** -- `hashicorp/setup-terraform` action.
2. **Terraform init** in `terraform/` directory.
3. **Terraform apply -auto-approve** -- deploys Worker, KV namespace, uploads
   `embeddings-bge.json` to KV.
4. Uses GitHub repo secrets: `CLOUDFLARE_API_TOKEN`, `CLOUDFLARE_ACCOUNT_ID`.

**Make the Terraform step conditional:** If Cloudflare secrets aren't configured,
skip the step gracefully (using `if: secrets.CLOUDFLARE_API_TOKEN != ''` or similar).
This way the existing Jekyll build + GitHub Pages deploy still works for anyone who
hasn't set up Cloudflare credentials yet.

**Sequencing:**
```
Jekyll build (generates embeddings.json + embeddings-bge.json)
  -> Terraform apply (deploys Worker + uploads bge embeddings to KV) [conditional]
  -> GitHub Pages deploy (deploys frontend with toggle)
```

**Why this step is safe:** The Terraform step is conditional -- if secrets aren't
configured, it's skipped and the rest of the pipeline works exactly as before.
The Jekyll build and GitHub Pages deploy are unaffected.

**Build verification:**
```
just build                    # Jekyll build must pass (unaffected)
# CI pipeline: push to a branch, verify the workflow runs
#   - Without Cloudflare secrets: Terraform step skips, rest works
#   - With Cloudflare secrets: Full pipeline deploys Worker + site
```

---

### Post-Deployment: Update Default Mode

After Step 6 is deployed and the Worker is confirmed working:

- Update `site/ask/index.html` to default to "Cloud" mode.
- Update the hardcoded Worker URL if it was a placeholder.

This is a small follow-up commit, not a full step.

---

## Key Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Relationship to existing | Additive, not replacement | User requirement; both options coexist |
| Dual embeddings | Two files, two models | Different embedding models produce incompatible vector spaces |
| Embedding storage | KV (single key) | Simple, free tier allows 100K reads/day, embeddings fit in one value (25MB limit) |
| Cloud embedding model | bge-small-en-v1.5 | Available on both Workers AI and locally via sentence-transformers, 384-dim |
| Text generation model | Llama 3.1 8B Instruct | Huge quality upgrade over distilgpt2, instruction-following, fits free tier |
| IaC tool | Terraform (not Wrangler) | User requirement; demonstrates IaC skills for interviews |
| CORS strategy | Allowlist GitHub Pages origin | Security best practice; no wildcard |
| Default mode | Cloud | Better answers, faster page load; in-browser remains available |
| Worker URL | `*.workers.dev` subdomain | Free, no custom domain needed, avoids DNS complexity |

---

## What Makes This Interview-Ready

1. **Production LLM deployment** -- Llama 3.1 8B running on Cloudflare's global edge
   network, serving real traffic.
2. **Infrastructure as Code** -- Full Terraform configuration, reproducible deployments.
3. **RAG architecture** -- Semantic search over real content with retrieval-augmented
   generation. Industry-standard pattern.
4. **CI/CD pipeline** -- Automated build, embed, deploy cycle via GitHub Actions.
5. **Cost engineering** -- Designed to run entirely within free tier limits.
6. **Production concerns** -- CORS security, error handling, structured API responses.
7. **Side-by-side comparison** -- Users can compare in-browser vs cloud LLM quality,
   which makes for a compelling demo.

---

## Risks and Mitigations

| Risk | Mitigation |
|------|------------|
| Embeddings JSON too large for KV value (>25MB) | ~155 posts with 384-dim floats should be ~2-5MB. Monitor size. |
| 10ms CPU time limit on free tier | Cosine similarity over ~500 chunks should be well under 10ms. AI calls are I/O wait, not CPU. |
| 10,000 neurons/day limit | Personal site traffic is low. Each query costs ~100-200 neurons (embedding + generation). Budget for ~50-100 queries/day. |
| Terraform Cloudflare provider v5 API changes | Pin provider version in `required_providers`. |
| Dual embedding generation doubles build time | Embedding generation is fast (~30s per model). Acceptable tradeoff. |
| `bge-small-en-v1.5` local vs Workers AI slight differences | Both use the same BAAI model weights. Test similarity scores after deployment. |

---

## Out of Scope (for now)

- Streaming responses (would require SSE/WebSocket -- keep it simple with JSON)
- Custom domain for the Worker (workers.dev is fine)
- Rate limiting beyond Cloudflare's built-in free tier limits
- Vectorize (Cloudflare's vector DB) -- KV is simpler and sufficient for this scale
- Caching responses -- could add later with Workers Cache API
