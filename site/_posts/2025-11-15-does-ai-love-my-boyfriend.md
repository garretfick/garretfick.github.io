---
layout: post
title: Does AI love my boyfriend?
date: 2025-11-15
---

*Note: This blog post was adapted from a Toastmasters Q&A session I gave.*

A friend once asked me "why does ChatGPT hate my boyfriend"?

This is not a therapy session. Instead, I will endeavor to answer the question. More precisely, I will give you the tools so that you can answer this question for her.

We need to begin somewhere. I'm sure every one of you has been in a conversation where you felt like you knew exactly the next word someone was going to say. You were able to predict one word.

The essential capability of AI is predicting the next word in a sequence. AI does this one word at a time. So, if you were to ask AI "Write a scathing, mean email to my boss.", AI takes your input and then predicts the next word.  It might be  "no", or some other word.

Conceptually, building a word predictor is relatively simple and mysterious. What we do is take lots of text, books, websites, scientific articles, news reports and an so on. Then we create a special kind of computer program that predicts words using that content. For example, given the phrase, "the only thing we have to fear is .." the program predicts "fear" then "itself". Creating this kind of computer program takes months and is very expensive. Additionally, this program, in it's raw form, and has some pretty big limitations. I'd like to talk about two problems - safety and relevance - because understanding those limitations and how we overcome them will help explain AIs behaviour.

This computer program lacks any kind of moral compass or alignment with human values.  You could as for information on how to commit crimes, self-harm, biased, toxic, and so on. As a word predictor, this program would give answers that many people would think are inappropriate. When then refine the word predictor. In essence, people rank answers, and we use this to fine-tune the word predictor. This process of fine-tuning the output to avoid undesired responses causes the AI to be more agreeable - to generate responses biased to human preferences - even at the expense of truthfulness.

The second problem in part due to the long time it takes to create the computer program. That quote, the only thing we have to fear is fear itself, was spoken March 4, 1933. Suppose it is one day later, March 5. Well, our word predictor doesn't yet have examples of text with the quote. After all, news articles haven't been written yet. It would take months before we could make an update word predictor that has this text. This kind of computer program would be forever out of date.

We solve this problem by giving AI more text than what you type. For example, Gemini might first search the web for similar content, then include content from the search results into your query. In this way, AI can quick learn about news as it happens, tell you the weather. Gemini doesn't just use news articles. It also uses content from your previous chat sessions and perhaps other data, thereby customizing it's response just for you.

So we have a computer program that's designed to be agreeable and uses our past conversations. But why does that make this friend think it hates her boyfriend?

The answer is that humans are designed to see human characteristics everywhere in our environment, whether it is animals, weather, even computers.

Let me give you an example, a computer scientist designed ELIZA, a simple computer program designed to act like a Rogerian psychotherapist. A Rogerian psychotherapist responds by echoing back what you say. For example, suppose you say, "I messed up again.  I'm terrified of losing her", the therapist might respond with "You're terrified of losing her because of the mistake you made."

A funny thing happened. The scientists own secretary asked him to leave the room so that she could have a private conversation with the simple computer program he had created. She was treating the program as though it were a human therapist.

So, let's put all this together and try to answer the question.

AI's nothing more than a word predictor. It lacks emotion. But, it is tuned to be agreeable, so if the friend regularly complain about her boyfriend, it will agree with her. And AI is using the chat history, including all the things she has said about her boyfriend in the past. Finally, we are wired to see human-like emotions even in computer programs.

