---
layout: default
title: Ask
---

<p>
    You can ask me about things on my website. I'm doing by best to answer the questions.
</p>

<p class="admonition">Asking me is an experiment in using AI. The answers are not very good.</p>

    <form class="pure-form">
        <fieldset class="pure-group">
            <select id="mode" class="pure-input-1-4">
                <option value="browser">In-Browser (distilgpt2)</option>
                <option value="cloud">Cloud (Llama 3.1 8B)</option>
            </select>
            <textarea class="pure-input-2-3" id="question" placeholder="What is the main topic of the tech blog post?"></textarea>
            <button class="pure-button pure-input-1-4 pure-button-primary" id="askButton">Ask</button>
        </fieldset>
    </form>

<div class="pure-g">
    <div class="pure-u-1">
        <p id="status">Ready</p>
    </div>

    <div class="pure-u-1">
        <p id="output">The answer will appear here.</p>
    </div>
</div>

<script type="module">
    // 1. Import the 'pipeline' function from the Transformers.js library.
    import { pipeline, cos_sim } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2';

    const WORKER_URL = 'https://ask-ai.ACCOUNT_ID.workers.dev';

    // 2. Get references to the HTML elements
    const questionInput = document.getElementById('question');
    const askButton = document.getElementById('askButton');
    const statusDiv = document.getElementById('status');
    const outputDiv = document.getElementById('output');
    const modeSelect = document.getElementById('mode');

    // 3. Global variable to hold the blog content and embeddings
    let blogContent = null;

    // Caching models for performance
    let embedder = null;
    let generator = null;

    // 4. Function to load the embeddings file (called lazily on first browser-mode question)
    async function loadEmbeddings() {
        statusDiv.textContent = 'Loading knowledge base...';
        const response = await fetch('/static/model/embeddings-all-MiniLM-L6-v2.json');
        blogContent = await response.json();
        console.log('Embeddings loaded.');
    }

    // 5. Answer using the in-browser Transformers.js pipeline
    async function answerWithBrowser() {
        const query = questionInput.value;
        if (!query) {
            outputDiv.textContent = 'Please ask a question.';
            return;
        }

        askButton.disabled = true;

        // Lazy-load embeddings on first use
        if (!blogContent) {
            await loadEmbeddings();
        }

        // Step A: Generate an embedding for the user's question
        statusDiv.textContent = 'Thinking... (creating query embedding)';
        if (!embedder) {
                embedder = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');
        }
        const queryEmbedding = await embedder(query, { pooling: 'mean', normalize: true });

        // Step B: Find the most similar chunks from the blog content
        statusDiv.textContent = 'Thinking... (searching for relevant content)';
        const rankedChunks = blogContent.map(doc => {
            const similarity = cos_sim(queryEmbedding.data, doc.embedding);
            return { ...doc, similarity };
        }).sort((a, b) => b.similarity - a.similarity);

        // Select the top 3 most relevant chunks
        const topChunks = rankedChunks.slice(0, 3);
        const context = topChunks.map(chunk => chunk.chunk).join('\n\n---\n\n');

        // Step C: Generate an answer using the retrieved context
        statusDiv.textContent = 'Thinking... (generating final answer)';

        console.log(`${context}`);

        // Construct a clear prompt for the generative model
        const prompt = `
Based SOLELY on the following context, please provide a direct answer to the question. Do not use any outside knowledge.

Context:
${context}

Question: ${query}

Answer:`;

        if (!generator) {
            generator = await pipeline('text-generation', 'Xenova/distilgpt2');
        }

        const result = await generator(prompt, {
            max_new_tokens: 200,
            no_repeat_ngram_size: 3,
        });

        // Step D: Display the final answer
        outputDiv.textContent = result[0].generated_text.replace(prompt, '').trim();
        statusDiv.textContent = 'Ready to answer questions.';
        askButton.disabled = false;
    }

    // 6. Answer using the Cloudflare Worker
    async function answerWithCloudflare() {
        const query = questionInput.value;
        if (!query) {
            outputDiv.textContent = 'Please ask a question.';
            return;
        }

        askButton.disabled = true;
        statusDiv.textContent = 'Thinking...';

        try {
            const response = await fetch(WORKER_URL + '/ask', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ question: query }),
            });

            if (!response.ok) {
                throw new Error(`Server responded with ${response.status}`);
            }

            const data = await response.json();
            outputDiv.textContent = data.answer;
        } catch (error) {
            console.error('Cloud request failed:', error);
            outputDiv.textContent = 'Cloud service is unavailable. Try the in-browser option.';
        }

        statusDiv.textContent = 'Ready to answer questions.';
        askButton.disabled = false;
    }

    // 7. Dispatch based on selected mode
    askButton.addEventListener('click', () => {
        if (modeSelect.value === 'cloud') {
            answerWithCloudflare();
        } else {
            answerWithBrowser();
        }
    });
</script>
